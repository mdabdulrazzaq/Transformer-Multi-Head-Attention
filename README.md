
# Understanding Self-Attention in Transformers

This repository provides an in-depth look into the mechanics of **Self-Attention in Transformers**. 
Through detailed examples and code, you'll understand how embeddings are contextualized to capture dependencies between tokens.

## What's Inside
- **Core Attention Mechanism**: Linear projections, scaled dot product, and attention weighting.
- **Examples**: Demonstrations of single-head and multi-head attention mechanisms.

## Usage
Clone the repository and run examples:

```bash
python examples/toy_example.py
```

```bash
python examples/multihead_demo.py
```
